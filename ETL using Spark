#Installing Required Libraries
!pip install pyspark==3.1.2 -q
!pip install findspark -q

#Importing Required Libraries
# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# FindSpark simplifies the process of using Apache Spark with Python

import findspark
findspark.init()

from pyspark.sql import SparkSession

#Create SparkSession
#Ignore any warnings by SparkSession command

spark = SparkSession.builder.appName("ETL using Spark").getOrCreate()

#Create a Dataframe from the raw data and write to CSV file
#create a list of tuples
#each tuple contains the student id, height and weight
data = [("student1",64,90),
        ("student2",59,100),
        ("student3",69,95),
        ("",70,110),
        ("student5",60,80),
        ("student3",69,95),
        ("student6",62,85),
        ("student7",65,80),
        ("student7",65,80)]

# some rows are intentionally duplicated

##create a dataframe using createDataFrame and pass the data and the column names.

df = spark.createDataFrame(data, ["student","height_inches","weight_pounds"])

# show the data frame

df.show()

#Write to csv file
df.write.mode("overwrite").csv("student-hw.csv", header=True)

#Verify the csv file
# Load student dataset
df = spark.read.csv("student-hw.csv", header=True, inferSchema=True)

# display dataframe
df.show()

#Read from a csv file and write to parquet file
# Load student dataset
df = spark.read.csv("student-hw.csv", header=True, inferSchema=True)

# display dataframe
df.show()

#Drop Duplicates
df = df.dropDuplicates()

df.show()

Drop Null values

df=df.dropna()

#Observe the rows with null values getting dropped
df.show()

#Save to parquet file
#Write the data to a Parquet file
df.write.mode("overwrite").parquet("student-hw.parquet")

# verify that the parquet file(s) are created
!!ls -l student-hw.parquet

#Condense PARQUET to a single file
#Reduce the number of partitions in the dataframe to one.

df = df.repartition(1)

#Save to parquet file
#Write the data to a Parquet file
df.write.mode("overwrite").parquet("student-hw-single.parquet")

# verify that the parquet file(s) are created
!ls -l student-hw-single.parquet

#Read from a parquet file and write to csv file
df = spark.read.parquet("student-hw-single.parquet")

df.show()

#Transform the data
#import the expr function that helps in transforming the data
from pyspark.sql.functions import expr

#Convert inches to centimeters
# Convert inches to centimeters
# Multiply the column height_inches with 2.54 to get a new column height_centimeters
df = df.withColumn("height_centimeters", expr("height_inches * 2.54"))
df.show()

#Convert pounds to kilograms
# Convert pounds to kilograms
# Multiply weight_pounds with 0.453592 to get a new column weight_kg
df = df.withColumn("weight_kg", expr("weight_pounds * 0.453592"))
df.show()

#Drop the columns
# drop the columns "height_inches","weight_pounds"
df = df.drop("height_inches","weight_pounds")
df.show()

#Rename a column
# rename the lengthy column name "height_centimeters" to "height_cm"
df = df.withColumnRenamed("height_centimeters","height_cm")
df.show()

#Save to csv file
df.write.mode("overwrite").csv("student_transformed.csv", header=True)

#Verify the csv file
# Load student dataset
df = spark.read.csv("student_transformed.csv", header=True, inferSchema=True)
# display dataframe
df.show()

#Stop Spark Session
spark.stop()
